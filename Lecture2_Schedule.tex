%        File: Lecture2_Schedule.tex
%     Created: Fri Sep 11 04:00 PM 2015 EDT
% Last Change: Fri Sep 11 04:00 PM 2015 EDT
%
\documentclass[12pt]{article}
% Package for non-breaking words
\usepackage[none]{hyphenat}
\usepackage[margin=1in]{geometry}

% For standard math
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
%Set up the margins
%\usepackage[left=.7in,right=.7in,top=.7in,bottom=.7in]{geometry}
\usepackage{amssymb,amsmath,amsthm,mathrsfs,verbatim,upgreek}
\usepackage{extarrows}%% For extending the equal sign when there is   \xlongequal{<stuff>}
%\sout{} strike through!
\usepackage{marginnote}%\marginnote{}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
%Create a header at the top of every page
%\usepackage{fancyhdr}
%\pagestyle{fancy}
\usepackage{pdfpages} %merge pdf with: \includepdfmerge{heine-borel_proof.pdf,-}
\usepackage{graphicx}%to input images \includegraphics[width=130mm]{Open_Open}
\usepackage{lipsum}
\usepackage{color}
%You can define commands for the things you use frequently.
\newcommand{\al}{\aleph}
\newcommand{\A}{\mathcal{A}}
\renewcommand{\c}{\mathfrak c}
\newcommand{\C}{{\mathbb C}}
\newcommand{\D}{\mathcal D}
\newcommand{\E}{\mathfrak E}
\renewcommand{\L}{{\mathcal L}}
\newcommand{\M}{\mathcal M}
\newcommand{\N}{{\mathbb N}}
\renewcommand{\P}{\mathcal {P}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\X}{{\mathlarger{\mathcal X}}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\To}{\Rightarrow}
\newcommand{\WLOG}{With out loss of generality}
\newcommand{\card}{\text{card }}

% The limits.
\newcommand{\limk}{\underset{k\to\infty}\lim}
\newcommand{\limn}{\underset{n\to\infty}\lim}
\newcommand{\liminfn}{\underset{n\to\infty}{\underline{\lim}}}
\newcommand{\liminfk}{\underset{k\to\infty}{\underline{\lim}}}
\newcommand{\liminfp}{\underset{p\to\infty}{\underline{\lim}}}
\newcommand{\liminfj}{\underset{j\to\infty}{\underline{\lim}}}
\newcommand{\limsupn}{\underset{n\to\infty}{\overline{\lim}}}
\newcommand{\limsupk}{\underset{k\to\infty}{\overline{\lim}}}
\newcommand{\limsupp}{\underset{p\to\infty}{\overline{\lim}}}
\newcommand{\limsupr}{\underset{r\to\infty}{\overline{\lim}}}
\newcommand{\sgn}{\text{sgn}}
% For L^p space
\newcommand{\Linfty}{{L^\infty(E)}}
\newcommand{\Lp}{{L^{p}(E)}}
\newcommand{\Lq}{{L^{q}(E)}}
\usepackage{mdframed}%needed for box like theorems.\newmdtheoremenv %In the box, the footnotes are more handy! ^.^
% Additional structures, enumerated through section number.
% Found as of 2015-06-30
% Fonts in the environments will be normal (standing straight up)
\theoremstyle{definition}
% Define all the theorem-based environments.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{THM}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{claimbox}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{cookbook}[theorem]{Cookbook}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
%\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem*{exercise_nonum}{Exercise}
\newtheorem{fact}[theorem]{Facts}
\newtheorem{idea}[theorem]{Idea}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{question}[theorem]{Question}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remark_box}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newmdtheoremenv{example}{Example}
\newmdtheoremenv{question_sqrt}{Question}
\newmdtheoremenv{typo}{Typo Correction}
% Found as of 2015-06-30
% Fonts will resume to be italic in for the environments thereafter.
\theoremstyle{plain}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem*{defn}{Definition}
\newtheorem*{rudin}{Rudin Says}
\newtheorem*{problem}{\textcolor[rgb]{1.00,0.00,0.00}{Problem}}
\usepackage{mathrsfs} % enable people to use \mathscr{A}
\usepackage{color}
\usepackage{xcolor}
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
\newcommand{\highlight}[1]{%
 \colorbox{yellow!50}{$\displaystyle#1$}}

\usepackage{bbm}
\usepackage{relsize} % For large symbles: \mathlarger{math_expression}
\usepackage{marvosym} %
\usepackage{enumerate}
%\usepackage{enumitem} % For using: \begin{itemize}[leftmargin=-.5in]
%\usepackage{comment}
% To make footnote numering by section.
\makeatletter
\@addtoreset{footnote}{section}
\makeatother
\usepackage{colonequals}

% Package for including footnotes to section-titles
\usepackage[stable]{footmisc}

% Highlighting:
\usepackage{xcolor}
\usepackage{newverbs}
\newverbcommand{\bverb}
{\begin{lrbox}{\verbbox}}
{\end{lrbox}\colorbox{blue!30}{\box\verbbox}}
\newverbcommand{\yverb}
{\begin{lrbox}{\verbbox}}
{\end{lrbox}\colorbox{yellow!50}{\box\verbbox}}
\newverbcommand{\gverb}
{\begin{lrbox}{\verbbox}}
{\end{lrbox}\colorbox{green!30}{\box\verbbox}}
\newverbcommand{\rverb}
{\begin{lrbox}{\verbbox}}
{\end{lrbox}\colorbox{red!30}{\box\verbbox}}
\newverbcommand{\grayverb}
{\begin{lrbox}{\verbbox}}
{\end{lrbox}\colorbox{gray!30}{\box\verbbox}}

% For := symbol \coloneqq
\usepackage{mathtools}

% For large \mid (as conditional)
\usepackage{mleftright}
%\[
%L = \sup \mleft\{\, \sum_{x \in F} a(x) \;\middle|\; F \subset X,\, |F| < \infty \,\mright\},
%\]
\newcommand{\suchthat}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}




% Check-marks
\usepackage{pifont} % http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Strike through
\usepackage[normalem]{ulem}
% Example: \sout{Hello World}

% Putting a box around paragraph
\usepackage{mdframed}
% \begin{mdframed}

% Putting a bar below: \barbelow{something}
\usepackage{stackengine}
\newcommand\barbelow[1]{\stackunder[1.2pt]{$#1$}{\rule{.8ex}{.075ex}}}


\title{Schedule for Week 2 of SI 701: \\Information Theory and Artificial
Intelligence}
\author{Meghana \& Linfeng }


\begin{document}
\maketitle

The goal is that we, as tomorrow's ``PhDs in Information'', should know enough about information theory and its origins to converse about it in casual situations or interviews.

\section{Part 1: while Paul is in the room}
For the first 80 minutes, Paul will be around.

\subsection{Opening: summary and stories (30 min)}
\subsubsection{Summarize the readings}
\subsubsection{Write/talk about the two things that you found most interesting in this week's readings (10-15 min)}
Get people to talk.

\subsubsection{What's new: (check list)}
\begin{itemize}
    \item Entropy: measure for information.
        \begin{itemize}
            \item A measure of information with units as `bits'.
            \item More technical details comes after the break. %When Paul is gone.

        \end{itemize}
    \item Diagram of Information Source, Transmitter, Noise Source, Receiver,
        Destination;
        \begin{itemize}
            \item Compression Lossy vs Lossless
                \begin{enumerate}
                    \item  Summary vs Excerpt
                    \item Diff (for storage and version control)
                \end{enumerate}
        \end{itemize}
    \item Further discussion on applications/understandings of Entropy (Need to
        post these questions and lead the discussion)
        \newpage
        \begin{itemize}
            \item When a message is not yet random, it contains redundancy.
                \begin{itemize}
                    \item Compression reaches zero redundancy when the message
                        is truly random;
                    \item Encryption achieves the stage of ``undecipherable'' if
                        the message is truly random.
                    \item Then, does lossless compression reach a fully-lossy
                        consequence when pushing it to the limit?
                \end{itemize}
            \item {Underdog wins}
            \item {Redundancy does not increase or decrease entropy}
        \end{itemize}
    \item Turing test as a standard for Artificial Intelligence (1950):
        \textbf{Can machines think?}
        \begin{enumerate}
            \item Describe the Turing machine & the universal Turing machine.
            \item Turing continued to ask “What can a machine not do?”
            \item What is the Turing Test?
            \item In \emph{The Most Human Human}, how does Brian Christian relate the Turing test to entropy?
        \end{enumerate}
\end{itemize}

\subsection{Paul's lecture (15 minutes)}
Paul agreed to cover:
\begin{itemize}
    \item compression
    \item Comparison among: completing texts (texting on iPhone) and generating
        text (Turing test)
\end{itemize}



\subsection{Nicole's lecture + connecting with future materials (15 minutes)}
\begin{itemize}
    \item Modern communication theory's critique of Shannon's information
        theory.
\end{itemize}

\section{Part 2: second 1.5 hours}
Resume after a 10 minute break.
\subsection{On definition of Entropy (20min) }
        \begin{itemize}
            \item Foundation: discrete probability distribution and probability density function;
        \end{itemize}
\begin{definition}
    For discrete distribution,
    \[
        H(\tilde x) = - \sum p_i \log_{2} p_i
    \]
\end{definition}

\subsubsection{About the probability distribution:}
When calculating the entropy of a random variable, we only use ``true
probability''.

%\subsubsection{Measure of surprisal? One that is defined over subjectively
%assigned information.}
%Surprisal defined on Wiki has a functional form that resembles entropy. For the
%coin-flipping case, the value is exactly the same. I doubt if surprisal would
%really serve the need for comparison.


\subsection{Game on word prediction (20 min)}
Would a human being be better at word prediction? Let's play a version of the Shannon game.
\begin{enumerate}[{Stage} 1]
    \item (\textbf{Setup}) Pair up into groups of 2;
    \item (\textbf{Seeding}) Person 1 comes up with a sentence. They then type the
        first word into their smartphone;
    \item (\textbf{Prediction}) We now compare between the following three: the smartphone's prediction of the next word; the person 2 (of the pair) prediction by ``guessing''; and the actual word that person 1 had in mind. Repeat this for the whole sentence.
    \item (\textbf{Discussion}) Reflect on & share your experiences.
\end{enumerate}



%\subsection{Game: demonstrating the diagram in a concrete setting (25 minutes) }
%For designing the game, we may want to control for the following factors
%independently (i.e. introducing one factor at a time). Each game shall take
%around 15 minutes or so;
%\begin{itemize}
%    \item Compression: from sentence to graph, and then back to sentence;
%    \item Noise through the channel: plot a message, and have someone to doodle
%        on it (noise), and have the receiver decipher.
%\end{itemize}
%The goal is to mimic a controlled environment.
%\subsubsection{ Potential things to add to the Game }
%\begin{itemize}
%    \item Measure of ``entropy'' per message. $\implies$ This would need a more
%        carefully defined set of ``potential messages'', on top of which we
%        could derive the background distribution/density-function.
%    \item Demonstrate how noise would affect entropy?
%\end{itemize}

\subsection{Discussion}
\begin{itemize}
    \item How does text prediction affect our behaviour/communication patterns? 
    Moreover, text prediction perpetuates the biases contained in the training data. For example, UN Women had a famous ad campaign showing actual autocomplete results on entering the keywords ``women should'', ``women can't'', ``women need to be'' in Google's search box. Results: ``women need to be put in their place'', ``women can't drive'', ``women should stay at home''.
    What are the implications of such algorithms?

    \item What makes computers a possible channel of artificial
        intelligence?
        \footnote{Due to the fact that computers as binary calculators can
            practice arbitrary algorithm, as algorithms could all be decomposed
            into binary relations governed by conditions and coconditions
            (``if'' statement``), as well as logic connectives (``and, or,
            not'').
        }

    \item Shall a system/algorithm/mechanism that follows \textbf{(fixed) rules}
        be counted as intelligence?

    \item What would be the problem if computers turned out to be intelligent?

    \item Summarize the Ashley Madison case and ask for people's opinions.

    \item How would you connect this week's readings to those from last week?

\end{itemize}

\subsection{Closing thoughts: did anyone change their mind about what they found most interesting in this week's readings?}

\subsection{Extras, if we have too much time left}
\begin{itemize}
  \item Live chat with a bot! www.jabberwacky.com
\end{itemize}

\end{document}

Chronological Outline for SI701 Week 2 Page
Outline
1. Writing (the creation of symbol storage) catalyzed logic.
   1. Axioms, Signs, Formulas, Proofs
1. History of the pursuit for “Perfectly precise expression”
   1. Leibniz 1678- “A certain script of language that perfectly represents the relationships between our thoughts”
Question: What else do you know about Leibniz?
   1. Boole- Early 1800’s The Laws of Thought (Formalized expression)
   2. Babbage-  Mid 1800’s Difference Engine (Sought precision)
1. Melding logic and math.
   1. Bertrand Russell and Alfred North Whitehead: Formalists (also David Hilbert) Sought perfection “formal certainty”.
      1. Is mathematics complete?
      2. Is mathematics consistent?
      3. Is mathematics decidable?
Question: What does decidable mean?
1. Principia Mathematica 1910
   1. Russell and Whitehead used a formalized structure that tried to create a set of rules to that could be used to prove all mathematical truths.
      1. Obtained through the usage of Symbolic Logic
         1. Applying formal logic rules to math.
   1. Contrast between symbolism and ordinary language
      1. Language builds on knowledge and experience of real things
      2. Symbols in the PM system should not require this same kind of knowledge
      3. Syntactic versus semantic
         1. Syntax is concerned with the rules used for constructing, or  transforming the symbols and words of a language, as contrasted with the semantics of a language which is concerned with its meaning.
   1. Paradoxes were getting in the way
      1. This statement is false.
      2. Berry’s Paradox- The least integer not nameable in fewer than nineteen syllables. (The previous sentence is only 18 syllables.)
      3. Barber Paradox- Who shaves the barber? (Sets within sets paradox.)
   1. The PM would not allow self-reference or self containment.
   2. Gödel said with the PM “one can prove any theorem using nothing but a few mechanical rules”
1. Kurt Gödel
   1. Originally examined Metamathmatics- Math from the outside
   2. 1931 “[Gödel] was about to make the most important statement, prove the most important theorem about knowledge in the twentieth century. He was going to kill Russell’s dream of a perfect logical system” (Gleick pg.181)
   3. Shows that no complete and consistent system can exist.
   4. Gödel mapping.
      1. Every symbol within a statement can be translated into a numerical value which has relationships between them which makes it a statement about numbers. Takes a linguistic paradox and encodes it into number theory. “This statement cannot be proved”
      2. Setup the 1-1 correspondence to math and statements about math. Gödel uses prime numbers because of their uniqueness which allows these numbers to encode statements about other numbers.
   1. There are true statements about numbers that cannot be proven.
   2. Math cannot be complete and consistent.
1. Characteristica universalis “Numbers could encode all of reasoning”  (Gleick pg.185)
2. Alan Turing
   1. The feasibility of machines that could analyze symbols allows for the thought:
   2. Are all numbers computable? (On computable numbers... 1936)
      1. A number is computable if its decimal could be written down by a machine. (Gleick pg. 207)
      2. No machine at the time could actually compute.
      3. Using the typewriter (Invented in the 1860’s) as a framework Turing theorized a machine called U (universal machine).
         1. Tape- long strip, divided into squares, infinite, record on
         2. Symbols- written on tape, 1 per square, binary
         3. Scanner- Reads symbols ,only one at a time
         4. States- “States of mind”, Set of instructions for what to do when encountering certain symbols
      1. Turing shows that U through its algorithms could arrive at uncomputable or even paradoxical algorithms which never end.
      2. These broken algorithms which can be encoded into numbers prove that mathematics is undecidable. (Gleick pg.211)
Questions: What did I get wrong? What didn’t make sense? Why is this important?


Flashback!
Question: Does anyone else want to summarize Bell Telephones expansion into Bell Labs?
1. Bell Telephones
   1. Late 1800’s to the early 1900’s telephone’s rapidly became apart of the US landscape.
   2. Switch Board becomes necessary to accommodate
   3. Switches boards are seen as “electrical brains”
   4. Bell Labs was created out of necessity.
      1. Complexities multiplied which created the need for mathematicians
   1. Harry Nyquist 1920’s Tackling the physical components related to transmitting data.
   2. Ralph Hartley- Attempt to pin down the concept of information in transmission. “He was attempting to subtract human knowledge from the equation”!
   3. Normalization- People began to feel that it was natural to possess machines dedicated to sending and receiving messages. (Gleick pg.170)
1. Claude Shannon
   1. Shannon as a child liked to tinker and liked puzzles
   2. 1932 Shannon goes to UM (Go Blue!)
   3. 1936 Goes to MIT to work with Vannevar Bush
      1. “Mathematician is not a man who can manipulate figures... he is primarily an individual who is skilled in the use of Symbolic Logic” -Bush
   1. Shannon goes to Princeton in 1940 for PostDoc
      1. Princeton is a hub of activity in science, but the work there is not what shannon had hoped.
End flashback
1. WWII
   1. 1940 Gödel comes to Princeton to get out of Europe.
   2. 1940 Shannon leaves Princeton for Bell Labs hopefully to pursue the “transmission of intelligence”
   3. Vannevar Bush heading the National Defense Research Committee.
   4. 1942 Turing goes to America to aid the US in their cryptanalysis. He visits Bell Labs.
Question: How do you feel about the diversion of research towards the war effort?

1. Bell Labs
   1. Claude Shannon
      1. Machine gun tracking- reducing the “noise”
      2. Saw the work of Nyquist and Hartley as promising in the drive towards unification. “They were representing the whole world as symbols in electricity”
   1.  Shannon and Turing 1943
      1. Turing and Shannon were cryptanalysts
         1. Shannon- Encrypting voices
         2. Turing- Decrypting Enigma (In England not in US)
1. The Mathematical Theory of Communication - Shannon 1948
   1. “Scientifically evaluating ‘communication’” (Christian pg.222)
      1. Text prediction and text generation turn out to be mathematically equivalent.
      2. Binary Digit = bit... neat.
         1. Building block for the concept of electronic storage systems
      1. Hijacking Information
         1. “Had to eradicate ’meaning’” (Gleick pg.219)
Question: Why is it necessary to eradicate meaning in order to evaluate communication?
      1. Noise and Entropy!
Question: Who wants to explain Entropy? What questions do you have about it?



Discussion questions:
1. Would a human being be better at next word prediction?  We could test this in class…  half the class could use phones to do the word prediction on their phones, half the class could compete what they think the next word is.  just a version of the shannon game.
3. What type of experiences and information would make you more human that what you already are?
4. Christian offers movie previews as examples of entropy in excerpts. Let's watch this preview on Frozen and discuss how much entropy we find on it.
5. Both cryptanalysis and compression have an inversely proportional relation with redundancy. Why?
6. If scholarly forms of writing such as literature reviews are lossy compressions of larger datasets, how can we ensure such compressions retain a high information value?
7. Could shannon have done what he did if he spoke chinese?
8. How do you think a test would go where the computer makes a judgement about whether the other is human or machine?
9. Why is a lossy filter required to compress most information?
10. Which is more valuable, interactions which produce low changes in entropy,
    or high?  why?

